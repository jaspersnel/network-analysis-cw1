{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "application/json; charset=UTF-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "r = requests.get(\"https://api.pushshift.io/reddit/search/submission/?subreddit=politics&sort_type=score&sort=desc&after=1546300800&before=1577836800&size=500\")\n",
    "print(r.status_code)\n",
    "print(r.headers['content-type'])\n",
    "\n",
    "# Store data\n",
    "submissions = r.json()['data']\n",
    "df_sub = pd.DataFrame(submissions)\n",
    "\n",
    "df_sub.to_pickle('submissions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for submission in submissions:\n",
    "    comments = []\n",
    "\n",
    "    date = submission['created_utc']\n",
    "    r = requests.get(\"https://api.pushshift.io/reddit/search/comment/?subreddit=politics&sort=asc&after=\" + str(date) + \"&before=1577836800&link_id=\" + submission['id'] + \"&size=500\")\n",
    "\n",
    "    # As long as the API returns more data, keep getting more comments after the last comment we retrieved (sorted by date)\n",
    "    while r.status_code == 200 and len(r.json()['data']) > 0:\n",
    "        comments = comments + r.json()['data']\n",
    "        date = comments[-1]['created_utc']\n",
    "\n",
    "        r = requests.get(\"https://api.pushshift.io/reddit/search/comment/?subreddit=politics&sort=asc&after=\" + str(date) + \"&before=1577836800&link_id=\" + submission['id'] + \"&size=500\")\n",
    "        print(str(r.status_code) + '\\n' if r.status_code != 200 else '.', end = '')\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    # After getting all comments for a post, save them to create a point we can return to if necessary\n",
    "    df = pd.concat([df, pd.DataFrame(comments)])\n",
    "    df.to_pickle('all_comments.pkl')\n",
    "    \n",
    "#     print('')\n",
    "#     print(i)\n",
    "    i += 1\n",
    "    \n",
    "# Make sure to remove any duplicates\n",
    "df = df.loc[~df['id'].duplicated()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we already saved our comments\n",
    "all_comments = pd.read_pickle('all_comments.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicates\n",
    "all_comments = all_comments.loc[~all_comments['id'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Select important columns\n",
    "all_comments = all_comments.filter(['author', 'body', 'id', 'link_id', 'parent_id', 'score'])\n",
    "\n",
    "# remove 't3_' or 't1_' in references to other nodes \n",
    "for col in ['link_id', 'parent_id']:\n",
    "    all_comments[col] = all_comments[col].apply(lambda str: re.sub(r'^t\\d_', '', str))\n",
    "\n",
    "print(all_comments['parent_id'].apply(lambda x: 't3_' in x or 't1_' in x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments.to_pickle('all_comments.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
